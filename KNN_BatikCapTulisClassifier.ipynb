{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Dependencies\n",
    "\n",
    "from PIL import Image\n",
    "from skimage.feature import greycomatrix, greycoprops\n",
    "\n",
    "import glob\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Global variables\n",
    "\n",
    "training_data    = []\n",
    "validation_data  = []\n",
    "class_names      = []\n",
    "map_8bit_to_3bit = [i // 32 for i in range(256)]\n",
    "\n",
    "# Scaling\n",
    "glcm_mean_values         = []\n",
    "glcm_stdev_values        = []\n",
    "glcm_min_values          = []\n",
    "glcm_max_values          = []\n",
    "glcm_max_subs_min_values = []\n",
    "glcm_q1_values           = []\n",
    "glcm_q3_values           = []\n",
    "glcm_q3_subs_q1_values   = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Functions\n",
    "\n",
    "def load_img(img_path):\n",
    "    return Image.open(img_path).convert('L')\n",
    "\n",
    "def get_img_size(img):\n",
    "    return img.size\n",
    "\n",
    "def print_img(img):\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    \n",
    "def get_resized_img(img, dimension):\n",
    "    return img.resize(dimension)\n",
    "    \n",
    "def get_img_colors(img):\n",
    "    return list(img.getdata())\n",
    "\n",
    "def get_3bit_img_colors(img):\n",
    "    img_colors = get_img_colors(img)\n",
    "    \n",
    "    loop_count = 0\n",
    "    for img_color in img_colors:\n",
    "        img_colors[loop_count] = map_8bit_to_3bit[img_color]\n",
    "        \n",
    "        loop_count += 1\n",
    "        \n",
    "    return img_colors\n",
    "\n",
    "def get_img_matrix(img_colors):\n",
    "    img_matrix = []\n",
    "    \n",
    "    loop_count = 0\n",
    "    img_square_dimension = int(math.sqrt(len(img_colors)))\n",
    "    \n",
    "    for row in range(img_square_dimension):\n",
    "        temp_row = []\n",
    "        for col in range(img_square_dimension):\n",
    "            temp_row.append(img_colors[loop_count])\n",
    "            \n",
    "            loop_count += 1\n",
    "        img_matrix.append(temp_row)\n",
    "        \n",
    "    return img_matrix\n",
    "\n",
    "def get_img_features(img, scale_mode='non_scaled', glcm_components=['contrast', 'correlation', 'energy', 'homogeneity', 'ASM', 'dissimilarity']):\n",
    "    img_3bit_colors = get_3bit_img_colors(img)\n",
    "    img_matrix = get_img_matrix(img_3bit_colors)\n",
    "    \n",
    "    glcm_matrix = greycomatrix(img_matrix, distances=[1], angles=[0], levels=12, symmetric=False, normed=False)\n",
    "    \n",
    "    img_features = []\n",
    "    for glcm_component in glcm_components:\n",
    "        img_features.append(greycoprops(glcm_matrix, glcm_component)[0][0])\n",
    "        \n",
    "    scaled_img_features = get_scaled_img_features(img_features, scale_mode)\n",
    "        \n",
    "    return tuple(scaled_img_features)\n",
    "\n",
    "def get_scaled_img_features(img_features, scale_mode='non_scaled'):\n",
    "    if(scale_mode == 'non_scaled'):\n",
    "        return tuple(img_features)\n",
    "    else:\n",
    "        scaled_img_features = []\n",
    "        glcm_length = len(img_features)\n",
    "\n",
    "        for glcm_index in range(glcm_length):\n",
    "            if(scale_mode == 'standard_scaled'):\n",
    "                scaled_img_features.append((img_features[glcm_index] - glcm_mean_values[glcm_index]) / glcm_stdev_values[glcm_index])\n",
    "            elif(scale_mode == 'min_max_scaled'):\n",
    "                scaled_img_features.append((img_features[glcm_index] - glcm_min_values[glcm_index]) / glcm_max_subs_min_values[glcm_index])\n",
    "            elif(scale_mode == 'robust_scaled'):\n",
    "                scaled_img_features.append((img_features[glcm_index] - glcm_q1_values[glcm_index]) / glcm_q3_subs_q1_values[glcm_index])\n",
    "            else:\n",
    "                scaled_img_features.append(img_features[glcm_index])\n",
    "\n",
    "        return tuple(scaled_img_features)\n",
    "    \n",
    "# Modelling\n",
    "\n",
    "def init_model():\n",
    "    global training_data\n",
    "    global validation_data\n",
    "    \n",
    "    training_data = []\n",
    "    validation_data = []\n",
    "    \n",
    "def load_class_names(training_path):\n",
    "    global class_names\n",
    "    \n",
    "    class_names = [class_name for class_name in os.listdir(training_path)]\n",
    "            \n",
    "def insert_img_features_into_training_data(class_name, img_features):\n",
    "    global training_data\n",
    "    \n",
    "    row_tuple = (class_name,) + img_features\n",
    "    training_data.append(row_tuple)\n",
    "        \n",
    "def insert_img_features_into_validation_data(class_name, img_features):\n",
    "    global validation_data\n",
    "    \n",
    "    row_tuple = (class_name,) + img_features\n",
    "    validation_data.append(row_tuple)\n",
    "\n",
    "def load_preprocessed_img(img_path, dimension=(250, 250)):\n",
    "    img = load_img(img_path)\n",
    "    img = get_resized_img(img, dimension)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def get_single_glcm_feature(model, glcm_index):\n",
    "    subset = []\n",
    "    \n",
    "    for row in model:\n",
    "        subset.append(row[glcm_index])\n",
    "        \n",
    "    return subset\n",
    "\n",
    "def get_features_only_model(model):\n",
    "    subset = []\n",
    "    \n",
    "    for row in model:\n",
    "        subset.append(row[1:])\n",
    "        \n",
    "    return subset\n",
    "\n",
    "def get_standard_scaled_model(model):\n",
    "    global glcm_mean_values\n",
    "    global glcm_stdev_values\n",
    "    \n",
    "    standard_scaled_model = []\n",
    "    \n",
    "    if(len(model) > 0):\n",
    "        model_row_length  = len(model[0])\n",
    "        glcm_mean_values  = [statistics.mean(get_single_glcm_feature(model, i)) for i in range(1, model_row_length)]\n",
    "        glcm_stdev_values = [statistics.stdev(get_single_glcm_feature(model, i)) for i in range(1, model_row_length)]\n",
    "        \n",
    "        glcm_length = len(glcm_mean_values)\n",
    "        \n",
    "        for row in model:\n",
    "            temp_row = []\n",
    "            temp_row.append(row[0])\n",
    "            temp_row += list(get_scaled_img_features(row[1:], 'standard_scaled'))\n",
    "\n",
    "            standard_scaled_model.append(tuple(temp_row))\n",
    "    \n",
    "    return standard_scaled_model\n",
    "        \n",
    "        \n",
    "def get_min_max_scaled_model(model):\n",
    "    global glcm_min_values\n",
    "    global glcm_max_values\n",
    "    global glcm_max_subs_min_values\n",
    "    \n",
    "    min_max_scaled_model = []\n",
    "    \n",
    "    if(len(model) > 0):\n",
    "        model_row_length     = len(model[0])\n",
    "        glcm_min_values      = [min(model)[i] for i in range(1, model_row_length)]\n",
    "        glcm_max_values      = [max(model)[i] for i in range(1, model_row_length)]\n",
    "        \n",
    "        glcm_length = len(glcm_min_values)\n",
    "        glcm_max_subs_min_values = [glcm_max_values[i] - glcm_min_values[i] for i in range(glcm_length)]\n",
    "\n",
    "        for row in model:\n",
    "            temp_row = []\n",
    "            temp_row.append(row[0])\n",
    "            temp_row += list(get_scaled_img_features(row[1:], 'min_max_scaled'))\n",
    "                \n",
    "            min_max_scaled_model.append(tuple(temp_row))\n",
    "            \n",
    "    return min_max_scaled_model\n",
    "\n",
    "def get_robust_scaled_model(model):\n",
    "    global glcm_q1_values\n",
    "    global glcm_q3_values\n",
    "    global glcm_q3_subs_q1_values\n",
    "    \n",
    "    robust_scaled_model = []\n",
    "    \n",
    "    if(len(model) > 0):\n",
    "        features_only_model = get_features_only_model(model)\n",
    "        \n",
    "        model_row_length = len(model[0])\n",
    "        glcm_q1_values   = np.quantile(features_only_model, .25, axis=0)\n",
    "        glcm_q3_values   = np.quantile(features_only_model, .75, axis=0)\n",
    "        \n",
    "        glcm_length = len(glcm_q1_values)\n",
    "        glcm_q3_subs_q1_values = [glcm_q3_values[i] - glcm_q1_values[i] for i in range(glcm_length)]\n",
    "        \n",
    "        for row in model:\n",
    "            temp_row = []\n",
    "            temp_row.append(row[0])\n",
    "            temp_row += list(get_scaled_img_features(row[1:], 'robust_scaled'))\n",
    "                \n",
    "            robust_scaled_model.append(temp_row)\n",
    "            \n",
    "    return robust_scaled_model\n",
    "    \n",
    "def load_data(training_path, validation_path, data_path, model_name, img_type='*.jpg', scale_mode='non_scaled', glcm_components=['contrast', 'correlation', 'energy', 'homogeneity', 'ASM', 'dissimilarity'], is_skip=False):\n",
    "    global training_data\n",
    "    global validation_data\n",
    "    \n",
    "    print('Loading data...')\n",
    "    \n",
    "    load_class_names(training_path)\n",
    "    \n",
    "    training_data_path   = data_path + model_name + '_' + scale_mode + '_training.data'\n",
    "    validation_data_path = data_path + model_name + '_' + scale_mode + '_validation.data'\n",
    "    \n",
    "    if(os.path.exists(training_data_path) and os.path.exists(validation_data_path) and not is_skip):\n",
    "        training_data_file = open(training_data_path, 'r')\n",
    "        training_data_str = training_data_file.read().split('\\n')\n",
    "        \n",
    "        for row in training_data_str:\n",
    "            row = row[1:len(row) - 1].split(', ')\n",
    "            row[1:] = [float(value) for value in row[1:]]\n",
    "            row_tuple = tuple(row)\n",
    "            training_data.append(row_tuple)\n",
    "        \n",
    "        validation_data_file = open(validation_data_path, 'r')\n",
    "        validation_data_str = validation_data_file.read().split('\\n')\n",
    "        \n",
    "        for row in validation_data_str:\n",
    "            row = row[1:len(row) - 1].split(', ')\n",
    "            row[1:] = [float(value) for value in row[1:]]\n",
    "            row_tuple = tuple(row)\n",
    "            validation_data.append(row_tuple)\n",
    "    else:\n",
    "        for class_name in class_names:\n",
    "            img_paths = glob.glob(training_path + class_name + '/' + img_type)\n",
    "\n",
    "            for img_path in img_paths:\n",
    "                img = load_preprocessed_img(img_path)\n",
    "                img_features = get_img_features(img, glcm_components)\n",
    "\n",
    "                insert_img_features_into_training_data(class_name, img_features)\n",
    "\n",
    "        for class_name in class_names:\n",
    "            img_paths = glob.glob(validation_path + class_name + '/' + img_type)\n",
    "\n",
    "            for img_path in img_paths:\n",
    "                img = load_preprocessed_img(img_path)\n",
    "                img_features = get_img_features(img, glcm_components)\n",
    "\n",
    "                insert_img_features_into_validation_data(class_name, img_features)\n",
    "                \n",
    "        if(scale_mode == 'standard_scaled'):\n",
    "            training_data   = get_standard_scaled_model(training_data)\n",
    "            validation_data = get_standard_scaled_model(validation_data)\n",
    "        elif(scale_mode == 'min_max_scaled'):\n",
    "            training_data   = get_min_max_scaled_model(training_data)\n",
    "            validation_data = get_min_max_scaled_model(validation_data)\n",
    "        elif(scale_mode == 'robust_scaled'):\n",
    "            training_data   = get_robust_scaled_model(training_data)\n",
    "            validation_data = get_robust_scaled_model(validation_data)\n",
    "                \n",
    "        if(not os.path.exists(data_path)):\n",
    "            os.mkdir(data_path)\n",
    "                \n",
    "        with open(training_data_path, 'w+') as file_writer:\n",
    "            training_data_last_index = len(training_data) - 1\n",
    "            for row_index in range(training_data_last_index):\n",
    "                file_writer.write(str(training_data[row_index]).replace('\\'', '') + '\\n')\n",
    "            file_writer.write(str(training_data[training_data_last_index]).replace('\\'', ''))\n",
    "                \n",
    "        with open(validation_data_path, 'w+') as file_writer:\n",
    "            validation_data_last_index = len(validation_data) - 1\n",
    "            for row_index in range(validation_data_last_index):\n",
    "                file_writer.write(str(validation_data[row_index]).replace('\\'', '') + '\\n')\n",
    "            file_writer.write(str(validation_data[validation_data_last_index]).replace('\\'', ''))\n",
    "            \n",
    "    print('--> Done\\n')\n",
    "    \n",
    "def train(training_rate=0.8):\n",
    "    print('Training...')\n",
    "    training_data_sample = []\n",
    "    testing_data_sample = []\n",
    "    \n",
    "    for row in training_data:\n",
    "        random_splitter = random.uniform(0, 1)\n",
    "\n",
    "        if(random_splitter <= training_rate):\n",
    "            training_data_sample.append(row)\n",
    "        else:\n",
    "            testing_data_sample.append(row)\n",
    "            \n",
    "    print('--> Done')\n",
    "    return training_data_sample, testing_data_sample   \n",
    "    \n",
    "def get_euclidean_distance(training_img_features, img_features):\n",
    "    distance = 0.0\n",
    "    \n",
    "    for glcm_index in range(6):\n",
    "        distance += (training_img_features[glcm_index] - img_features[glcm_index]) ** 2\n",
    "        \n",
    "    return math.sqrt(distance)\n",
    "\n",
    "def get_img_features_class(img_features, training_data_sample, k_neighbors=1):\n",
    "    minimum_training_img_distances = [-1 for i in range(k_neighbors)]\n",
    "    minimum_training_img_class_names = ['unknown' for i in range(k_neighbors)]\n",
    "    is_first_loop = True\n",
    "    for row in training_data_sample:\n",
    "        training_img_class_name = row[0]\n",
    "        training_img_features = row[1:]\n",
    "        \n",
    "        euclidean_distance = get_euclidean_distance(training_img_features, img_features)\n",
    "            \n",
    "        if(is_first_loop):\n",
    "            minimum_training_img_distances[0] = euclidean_distance\n",
    "            minimum_training_img_class_names[0] = training_img_class_name\n",
    "            is_first_loop = False\n",
    "        else:\n",
    "            for i in range(k_neighbors):\n",
    "                if(euclidean_distance < minimum_training_img_distances[i] or minimum_training_img_distances[i] == -1):\n",
    "                    for j in range(k_neighbors - 1, i, -1):\n",
    "                        minimum_training_img_distances[j] = minimum_training_img_distances[j-1]\n",
    "                        minimum_training_img_class_names[j] = minimum_training_img_class_names[j-1]\n",
    "                    minimum_training_img_distances[i] = euclidean_distance\n",
    "                    minimum_training_img_class_names[i] = training_img_class_name\n",
    "                    break\n",
    "    \n",
    "    minimum_training_img_class_names = list(filter(lambda val: val != 'unknown', minimum_training_img_class_names))\n",
    "    img_class_name = max(set(minimum_training_img_class_names), key=minimum_training_img_class_names.count)\n",
    "    \n",
    "    return img_class_name\n",
    "    \n",
    "def get_img_class(img, training_data_sample, k_neighbors=1, scale_mode='non_scaled', glcm_components=['contrast', 'correlation', 'energy', 'homogeneity', 'ASM', 'dissimilarity']):\n",
    "    img_features = get_img_features(img, scale_mode, glcm_components)\n",
    "    img_class_name = get_img_features_class(img_features, training_data_sample, k_neighbors)\n",
    "    \n",
    "    return img_class_name\n",
    "    \n",
    "def test(testing_data_sample, training_data_sample, k_neighbors=1):\n",
    "    print('Testing...')\n",
    "    total_correct_answer = 0\n",
    "    total_guess = 0\n",
    "    \n",
    "    for row in testing_data_sample:\n",
    "        expected_class_name = row[0]\n",
    "        test_img_features = row[1:]\n",
    "        \n",
    "        test_img_class_name = get_img_features_class(test_img_features, training_data_sample, k_neighbors)\n",
    "            \n",
    "        if(expected_class_name == test_img_class_name):\n",
    "            total_correct_answer += 1\n",
    "                \n",
    "        total_guess += 1\n",
    "    \n",
    "    accuracy = (total_correct_answer / total_guess) * 100\n",
    "    \n",
    "    print('--> Done, accuracy = ' + str(accuracy) + ' %')\n",
    "    \n",
    "    return accuracy\n",
    "    \n",
    "def validate(training_data_sample, k_neighbors=1):\n",
    "    print('Validating...')\n",
    "    total_correct_answer = 0\n",
    "    total_guess = 0\n",
    "    \n",
    "    for row in validation_data:\n",
    "        expected_class_name = row[0]\n",
    "        validation_img_features = row[1:]\n",
    "        \n",
    "        img_class_name = get_img_features_class(validation_img_features, training_data_sample, k_neighbors)\n",
    "            \n",
    "        if(expected_class_name == img_class_name):\n",
    "            total_correct_answer += 1\n",
    "                \n",
    "        total_guess += 1\n",
    "            \n",
    "    print('--> Done\\n')\n",
    "    \n",
    "    accuracy = (total_correct_answer / total_guess) * 100\n",
    "    \n",
    "    return accuracy\n",
    "    \n",
    "def save_model(k_neighbors, epochs, training_model, testing_model, model_path, model_name, scale_mode='non_scaled'):\n",
    "    knn_model_path = model_path + str(k_neighbors) + '/'\n",
    "    training_model_path = knn_model_path + model_name + '_' + scale_mode + '_training.model'\n",
    "    testing_model_path  = knn_model_path + model_name + '_' + scale_mode + '_validation.model'\n",
    "    model_epochs_path   = knn_model_path + model_name + '_' + scale_mode + '_epochs.model'\n",
    "    \n",
    "    if(not os.path.exists(model_path)):\n",
    "        os.mkdir(model_path)\n",
    "    \n",
    "    if(not os.path.exists(knn_model_path)):\n",
    "        os.mkdir(knn_model_path)\n",
    "                \n",
    "    with open(training_model_path, 'w+') as file_writer:\n",
    "        training_model_last_index = len(training_model) - 1\n",
    "        for row_index in range(training_model_last_index):\n",
    "            file_writer.write(str(training_model[row_index]).replace('\\'', '') + '\\n')\n",
    "        file_writer.write(str(training_model[training_model_last_index]).replace('\\'', ''))\n",
    "                \n",
    "    with open(testing_model_path, 'w+') as file_writer:\n",
    "        testing_model_last_index = len(testing_model) - 1\n",
    "        for row_index in range(testing_model_last_index):\n",
    "            file_writer.write(str(testing_model[row_index]).replace('\\'', '') + '\\n')\n",
    "        file_writer.write(str(testing_model[testing_model_last_index]).replace('\\'', ''))\n",
    "        \n",
    "    with open(model_epochs_path, 'w+') as file_writer:\n",
    "        file_writer.write(str(epochs))\n",
    "        \n",
    "    print('The best model has been saved.')\n",
    "        \n",
    "def get_saved_model(k_neighbors, model_path, model_name, scale_mode='non_scaled'):\n",
    "    print('Load previous best model in the same k_neighbors (k=' + str(k_neighbors) + ')...')\n",
    "    knn_model_path = model_path + str(k_neighbors) + '/'\n",
    "    training_model_path = knn_model_path + model_name + '_' + scale_mode + '_training.model'\n",
    "    testing_model_path  = knn_model_path + model_name + '_' + scale_mode + '_validation.model'\n",
    "    model_epochs_path   = knn_model_path + model_name + '_' + scale_mode + '_epochs.model'\n",
    "    \n",
    "    training_model = []\n",
    "    testing_model  = []\n",
    "    model_epochs   = 0\n",
    "    \n",
    "    if(os.path.exists(training_model_path) and os.path.exists(testing_model_path) and os.path.exists(model_epochs_path)):\n",
    "        training_model_file = open(training_model_path, 'r')\n",
    "        training_model_str = training_model_file.read().split('\\n')\n",
    "        \n",
    "        for row in training_model_str:\n",
    "            row       = row[1:len(row) - 1].split(', ')\n",
    "            row[1:]   = [float(value) for value in row[1:]]\n",
    "            row_tuple = tuple(row)\n",
    "            training_model.append(row_tuple)\n",
    "        \n",
    "        testing_model_file = open(testing_model_path, 'r')\n",
    "        testing_model_str = testing_model_file.read().split('\\n')\n",
    "        \n",
    "        for row in testing_model_str:\n",
    "            row       = row[1:len(row) - 1].split(', ')\n",
    "            row[1:]   = [float(value) for value in row[1:]]\n",
    "            row_tuple = tuple(row)\n",
    "            testing_model.append(row_tuple)\n",
    "            \n",
    "        model_epochs = int(open(model_epochs_path, 'r').read())\\\n",
    "        \n",
    "        print('--> Done, k=' + str(k_neighbors) + ', epochs=' + str(model_epochs) + '.\\n')\n",
    "    else:\n",
    "        print('--> Failed, model not found in the same k_neighbors (k=' + str(k_neighbors) + ')\\n')\n",
    "        \n",
    "    return training_model, testing_model, model_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "--> Done\n",
      "\n",
      "\n",
      "\n",
      "Load previous best model in the same k_neighbors (k=3)...\n",
      "--> Done, k=3, epochs=7.\n",
      "\n",
      "Validating...\n",
      "--> Done\n",
      "\n",
      "Best model accuracy\n",
      "Epochs   : 7\n",
      "Accuracy : 95.703125\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "Iteration 1\n",
      "=========================================\n",
      "Epoch 1\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 91.75257731958763 %\n",
      "\n",
      "Epoch 2\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 96.51741293532339 %\n",
      "\n",
      "Epoch 3\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 100.0 %\n",
      "\n",
      "Epoch 4\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 100.0 %\n",
      "\n",
      "Epoch 5\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 100.0 %\n",
      "\n",
      "Epoch 6\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 100.0 %\n",
      "\n",
      "Validating...\n",
      "--> Done\n",
      "\n",
      "Accuracy: 95.703125 %\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "Iteration 2\n",
      "=========================================\n",
      "Epoch 1\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 95.28301886792453 %\n",
      "\n",
      "Epoch 2\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 95.37037037037037 %\n",
      "\n",
      "Epoch 3\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 97.23926380368098 %\n",
      "\n",
      "Epoch 4\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 98.60788863109049 %\n",
      "\n",
      "Epoch 5\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 100.0 %\n",
      "\n",
      "Epoch 6\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 100.0 %\n",
      "\n",
      "Epoch 7\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 100.0 %\n",
      "\n",
      "Epoch 8\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 100.0 %\n",
      "\n",
      "Validating...\n",
      "--> Done\n",
      "\n",
      "Accuracy: 95.703125 %\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "Iteration 3\n",
      "=========================================\n",
      "Epoch 1\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 97.02970297029702 %\n",
      "\n",
      "Epoch 2\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 96.60194174757282 %\n",
      "\n",
      "Epoch 3\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 98.7460815047022 %\n",
      "\n",
      "Epoch 4\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 100.0 %\n",
      "\n",
      "Epoch 5\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 100.0 %\n",
      "\n",
      "Epoch 6\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 100.0 %\n",
      "\n",
      "Epoch 7\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 100.0 %\n",
      "\n",
      "Validating...\n",
      "--> Done\n",
      "\n",
      "Accuracy: 95.703125 %\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "Iteration 4\n",
      "=========================================\n",
      "Epoch 1\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 92.92035398230088 %\n",
      "\n",
      "Epoch 2\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 96.44444444444444 %\n",
      "\n",
      "Epoch 3\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 98.82697947214076 %\n",
      "\n",
      "Epoch 4\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 100.0 %\n",
      "\n",
      "Epoch 5\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 100.0 %\n",
      "\n",
      "Epoch 6\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 100.0 %\n",
      "\n",
      "Epoch 7\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 100.0 %\n",
      "\n",
      "Validating...\n",
      "--> Done\n",
      "\n",
      "Accuracy: 95.703125 %\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "Iteration 5\n",
      "=========================================\n",
      "Epoch 1\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 94.39252336448598 %\n",
      "\n",
      "Epoch 2\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 94.81132075471697 %\n",
      "\n",
      "Epoch 3\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 98.7878787878788 %\n",
      "\n",
      "Epoch 4\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 100.0 %\n",
      "\n",
      "Epoch 5\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 100.0 %\n",
      "\n",
      "Epoch 6\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 100.0 %\n",
      "\n",
      "Epoch 7\n",
      "Training...\n",
      "--> Done\n",
      "Testing...\n",
      "--> Done, accuracy = 100.0 %\n",
      "\n",
      "Validating...\n",
      "--> Done\n",
      "\n",
      "Accuracy: 95.703125 %\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Best Model\n",
      "Name     : cap_tulis_glcm_knn_non_scaled\n",
      "Index    : 0\n",
      "Epochs   : 6\n",
      "Accuracy : 95.703125\n",
      "NEW BEST MODEL!!!\n",
      "\n",
      "The best model has been saved.\n"
     ]
    }
   ],
   "source": [
    "#### Main\n",
    "\n",
    "# Props\n",
    "iterations           = 5\n",
    "epochs               = 20\n",
    "training_rate        = 0.8\n",
    "img_type             = '*.jpg'\n",
    "k_neighbors          = 3\n",
    "glcm_components      = ['contrast', 'correlation', 'energy', 'homogeneity', 'ASM', 'dissimilarity']\n",
    "perfect_test_overlap = k_neighbors + (k_neighbors // 2)\n",
    "scale_modes          = ['non_scaled', 'standard_scaled', 'min_max_scaled', 'robust_scaled']\n",
    "scale_mode_index     = 0\n",
    "is_skip_load         = True        \n",
    "\n",
    "# Defining model name\n",
    "model_name = 'cap_tulis_glcm_knn'\n",
    "\n",
    "# Defining paths\n",
    "root_path            = './'\n",
    "test_path            = root_path + 'test/'\n",
    "training_path        = root_path + 'training/'\n",
    "validation_path      = root_path + 'validation/'\n",
    "data_path            = root_path + 'data/'\n",
    "model_path           = root_path + 'model/'\n",
    "\n",
    "# Init\n",
    "init_model()\n",
    "load_data(training_path, validation_path, data_path, model_name, img_type, scale_modes[scale_mode_index], glcm_components, is_skip_load)\n",
    "best_model_accuracy  = 0.0\n",
    "best_model_index     = 0\n",
    "best_model_epochs    = 0\n",
    "training_data_sample = []\n",
    "testing_data_sample  = []\n",
    "is_previous_best_model_loaded = False\n",
    "\n",
    "# Load the saved previous best model\n",
    "print('\\n')\n",
    "previous_best_training_data_sample, previous_best_testing_data_sample, previous_best_model_epochs = get_saved_model(k_neighbors, model_path, model_name, scale_modes[scale_mode_index])\n",
    "if(len(previous_best_training_data_sample) > 0):\n",
    "    is_previous_best_model_loaded = True\n",
    "    best_model_accuracy = validate(previous_best_training_data_sample, k_neighbors)\n",
    "    best_model_epochs   = previous_best_model_epochs\n",
    "    training_data_sample.append(previous_best_training_data_sample)\n",
    "    testing_data_sample.append(previous_best_testing_data_sample)\n",
    "    \n",
    "    print('Best model accuracy')\n",
    "    print('Epochs   : ' + str(best_model_epochs))\n",
    "    print('Accuracy : ' + str(best_model_accuracy))\n",
    "    print('\\n\\n')\n",
    "    \n",
    "# Learning\n",
    "for iteration in range(iterations):\n",
    "    print('=========================================')\n",
    "    print('Iteration ' + str(iteration + 1))\n",
    "    print('=========================================')\n",
    "    data_sample_index = iteration + is_previous_best_model_loaded\n",
    "    training_data_sample.append([])\n",
    "    testing_data_sample.append([])\n",
    "    perfect_accuracy_count = 0\n",
    "    \n",
    "    epoch = 1\n",
    "    while(epoch <= epochs):\n",
    "        print('Epoch ' + str(epoch))\n",
    "        new_training_data_sample, new_testing_data_sample = train(training_rate)\n",
    "        training_data_sample[data_sample_index] += new_training_data_sample\n",
    "        testing_data_sample[data_sample_index]  += new_testing_data_sample\n",
    "        testing_accuracy = test(testing_data_sample[data_sample_index], training_data_sample[data_sample_index], k_neighbors)\n",
    "        print()\n",
    "        \n",
    "        if(testing_accuracy == 100.0):\n",
    "            perfect_accuracy_count += 1\n",
    "            if(perfect_accuracy_count == perfect_test_overlap):\n",
    "                break\n",
    "                \n",
    "        epoch += 1\n",
    "\n",
    "    model_accuracy = validate(training_data_sample[data_sample_index], k_neighbors)\n",
    "    \n",
    "    if(model_accuracy > best_model_accuracy):\n",
    "        best_model_accuracy = model_accuracy\n",
    "        best_model_epochs   = epoch\n",
    "        best_model_index    = data_sample_index\n",
    "    elif(model_accuracy == best_model_accuracy and epoch < best_model_epochs):\n",
    "        best_model_accuracy = model_accuracy\n",
    "        best_model_epochs   = epoch\n",
    "        best_model_index    = data_sample_index\n",
    "\n",
    "    print('Accuracy: ' + str(model_accuracy) + ' %')\n",
    "    print('\\n\\n')\n",
    "    \n",
    "    \n",
    "# Print the best model\n",
    "print('\\nBest Model')\n",
    "print('Name     : ' + model_name + '_' + scale_modes[scale_mode_index])\n",
    "print('Index    : ' + str(best_model_index - is_previous_best_model_loaded))\n",
    "print('Epochs   : ' + str(best_model_epochs))\n",
    "print('Accuracy : ' + str(best_model_accuracy))\n",
    "\n",
    "if(best_model_index == 0 and is_previous_best_model_loaded):\n",
    "    print('The best model still the previous best model.\\n')\n",
    "else:\n",
    "    print('NEW BEST MODEL!!!\\n')\n",
    "\n",
    "# Save the best model\n",
    "save_model(k_neighbors, best_model_epochs, training_data_sample[best_model_index], testing_data_sample[best_model_index], model_path, model_name, scale_modes[scale_mode_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cap\n"
     ]
    }
   ],
   "source": [
    "img = load_img(test_path + 'cap.jpg')\n",
    "print(get_img_class(img, training_data_sample[best_model_index], k_neighbors, scale_modes[scale_mode_index], glcm_components))\n",
    "# test correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tulis\n"
     ]
    }
   ],
   "source": [
    "img = load_img(test_path + 'cap2.jpg')\n",
    "print(get_img_class(img, training_data_sample[best_model_index], k_neighbors, scale_modes[scale_mode_index], glcm_components))\n",
    "# test false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tulis\n"
     ]
    }
   ],
   "source": [
    "img = load_img(test_path + 'tulis.jpg')\n",
    "print(get_img_class(img, training_data_sample[best_model_index], k_neighbors, scale_modes[scale_mode_index], glcm_components))\n",
    "# test correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
